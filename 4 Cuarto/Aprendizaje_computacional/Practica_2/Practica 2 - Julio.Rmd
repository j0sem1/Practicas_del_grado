---
title: "Clasificaci칩n de im치genes con redes neuronales"
subtitle: Segunda pr치ctica de Aprendizaje Computacional, Grado en Inform치tica (UMU)
author: "Jos칠 Miguel S치nchez Almagro"
date: "11/07/2020"
output: 
  html_document:
    theme: spacelab
    highlight: kate
    df_print: paged
    toc: true
    toc_float: true
    number_sections: true
#bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducci칩n

Durante el desarrollo de este documento se trabajar치 con el conjunto de fotograf칤as CIFAR-10.

CIFAR-10 es un conjunto de im치genes disponible en internet <https://www.cs.toronto.edu/~kriz/cifar.html> que contiene 60000 im치genes de 32x32 p칤xeles. Cada imagen de dicho conjunto se clasifica seg칰n lo que haya en la fotograf칤a, siendo 10 las posibles clasificaciones de cada fotograf칤a. El conjunto est치 perfectamente equilibrado entre dichas clases, es decir, hay 6000 im치genes de cada clase.
Las clases entre las que se clasifican las im치genes son:

* Avi칩n (airplane)
* Autom칩vil (automobile)
* P치jaro (bird)
* Gato (cat)
* Ciervo (deer)
* Perro (dog)
* Rana (frog)
* Caballo (horse)
* Barco (ship)
* Cami칩n (truck)


En esta pr치ctica se van a utilizar dos tecnolog칤as de redes neuronales para el reconocimiento y clasificaci칩n de las im치genes que se encuentran en el paquete CIFAR nombrado anteriormente. La primera tecnolog칤a ser치 la tradicional basada en un perceptr칩n multi-capa con no m치s de dos capas. La otra ser치 una red de convoluci칩n.

El objetivo es identificar qu칠 tecnolog칤a es capaz de producir los mejores modelos de reconocimiento y clasificaci칩n de im치genes.




# Datos

Dado que las sesenta mil fotograf칤as que contiene el conjunto original CIFAR-10 suponen un excesivo c칩mputo, se utilizar치 un conjunto reducido que solo contendr치 diez mil im치genes.

Cargamos dicho conjunto de im치genes desde dos archivos .csv.

```{r}
CIFAR10_10Kx = read.csv("CIFAR10_10Kx.csv")
str(CIFAR10_10Kx)
CIFAR10_10Ky = read.csv("CIFAR10_10Ky.csv")
str(CIFAR10_10Ky)
```

En CIFAR10_10kx tendremos las diez mil im치genes y en CIFAR10_10Ky la clase a la que pertenece cada fotograf칤a.



## Distribuci칩n de las im치genes

Sab칤amos que los ejemplos del conjunto de im치genes CIFAR-10 estaban perfectamente distribuidos pero no sabemos si ocurre lo mismo con este conjunto. A continuaci칩n, vemos cu치ntas im치genes tiene cada clase:

```{r, echo=FALSE}
cnts <- tabulate(CIFAR10_10Ky$x) 
names(cnts) <- 0:(length(cnts) - 1) 
cnts  
```

No est치 perfectamente distribuido, pero casi, por lo que la muestra es perfectamente v치lida. A continuaci칩n, visualizamos la misma informaci칩n en forma de gr치fica.

```{r, echo=FALSE}
bp = barplot(rbind(table(CIFAR10_10Ky$x)), main="Distribuci칩n de im치genes por clase", ylab="N칰mero de im치genes", xlab="Clases", col="purple")
```



## Conjuntos de training y test

Por requisitos de la pr치ctica se utilizar치 una estrategia hold-out 80-20 en la distribuci칩n de los datos. Esto es, el conjunto de diez mil im치genes se distribuir치 aleatoriamente en dos conjuntos:

* Conjunto de training. Contendr치 el 80% de las im치genes, y se utilizar치 para el entrenamiento de los modelos

* Conjunto de test. Contendr치 el 20% de las im치genes, y se utilizar치 para la validaci칩n de los modelos


Adem치s, se crear치n 10 divisiones, es decir, se crear치n 10 conjuntos de training y 10 de test. Esto se realiza con el fin de no entrenar cada modelo una sola vez y tener unos resultados m치s realistas y diversos. Se entrenar치 entonces cada modelo 10 veces y se obtendr치n 10 resultados distintos. El resultado final del modelo se obtendr치 realizando una media de dichos 10 valores.

```{r, echo=FALSE}
nConj = 10
```

El conjunto que contiene las im치genes ser치 el *x* y el que contiene la clasificaci칩n de las im치genes el *y*.


Vamos a observar la dimensi칩n de ambos conjuntos de datos:

```{r}
dim(CIFAR10_10Kx)
dim(CIFAR10_10Ky)
```

El conjunto de im치genes tiene diez mil im치genes con 3073 variables.
Siendo las im치genes de 32x32 p칤xeles, y teniendo cada p칤xel tres valores RGB, multiplicado tenemos 32 * 32 * 3 = 3072. Por lo tanto, cada variable representa un p칤xel en cada imagen, exceptuando la primera variable que indica el n칰mero de cada imagen.

El conjunto de clasificaci칩n tiene a su vez diez mil valores con dos variables. Una variable indica a que imagen nos referimos, como en el caso anterior, y la otra variable indica la clasificaci칩n de la imagen.


Una vez hemos entendido la distribuci칩n de los datos, hay que dividirlos en en diez conjuntos aleatoriamente. Para ello, utilizaremos la herramienta `createDataPartition` de la librer칤a de **Caret** que ya utilizamos en la pr치ctica anterior.

```{r}
# Cargamos la librer칤a de caret, necesitada para la funci칩n createDataPartition.
library(caret)
# Cargamos la librer칤a de keras para la funci칩n 'to_categorical'.
library(keras)

# Establecemos la semilla para obtener siempre las mismas divisiones.
set.seed(1234)

# Creamos una matriz con diez particiones de los datos.
Particiones <- createDataPartition(CIFAR10_10Kx$X,
                               p=0.8,              # Genera un 80% para train, 20% para test
                               list = TRUE,        # Dame los resultados en una matriz
                               times = nConj)      # Genera 10 particiones 80/20
```


Todas las im치genes y su correspondiente clasificaci칩n que se encuentran en las particiones de la matriz **Particiones** deben ser almacenadas en otra estructura, para utilizar las im치genes de una manera m치s c칩moda y sencilla en las redes neuronales. Adem치s, aprovecharemos para formatear la informaci칩n en un formato que acepten dichas redes.

Esto ser치 almacenado en una lista llamada **datos**, que a su vez contendr치 dos listas: una para datos de training y otra para test. Ambas listas tendr치n su correspondiente divisi칩n en *x* e *y*.

```{r}
# Eliminamos la primera variable, ya que no nos interesa
CIFAR10_10Kx$X = NULL

# Lista datos
datos = list(train = list(x = list(), y = list()), test = list(x = list(), y = list()))

# Bucle que almacena las im치genes de cada partici칩n en cada lista correspondiente. El primer 칤ndice distingue entre 'training' y 'test'. El segundo entre 'x' e 'y'. El tercero distingue entre particiones.
for (i in 1:nConj){
  datos[[1]][[1]][[i]] = data.matrix(CIFAR10_10Kx[unlist(Particiones[i], FALSE, FALSE),]) / 255
  datos[[1]][[2]][[i]] = to_categorical(CIFAR10_10Ky[unlist(Particiones[i], FALSE, FALSE),]$x, nConj)
  datos[[2]][[1]][[i]] = data.matrix(CIFAR10_10Kx[-unlist(Particiones[i], FALSE, FALSE),]) / 255
  datos[[2]][[2]][[i]] = to_categorical(CIFAR10_10Ky[-unlist(Particiones[i], FALSE, FALSE),]$x, nConj)
}
```

```{r}
dim(datos[[2]][[1]][[1]])
```


Para simplificar el proceso de despu칠s, se han realizado ciertas modificaciones sobre los conjuntos de datos para que est칠n correctamente formateados y poder usarlos con **Keras**.

**El data.frame que conten칤a la informaci칩n de las im치genes se ha convertido a una matriz, es decir, un array bidimensional. Adem치s, normaliza los colores en [0,1] dividi칠ndolos entre 255.**

**La variable que conten칤a la informaci칩n de clasificaci칩n de las im치genes se ha convertido un formato que acepta keras con la funci칩n `to_categorical()`. Esta, convierte la informaci칩n sobre a qu칠 clase pertenece cierta imagen en una opci칩n binaria, siendo 1 si pertenece a una clase determinada o 0 si no.**

A la lista datos se podr치 acceder mediante los 칤ndices o los nombres de las listas,. Es decir, con las siguientes dos 칩rdenes estar칤amos accediendo a la misma informaci칩n.

```{r, eval=FALSE}
datos$train$y[[1]]
datos[[1]][[2]][[1]]
```




# Redes neuronales de perceptr칩n multicapa (MLP)

Una red neuronal de perceptr칩n multicapa es un tipo de red neuronal que re칰ne perceptrones en diferentes capas conectadas de principio a fin. Tenemos tres tipos de capas en estas redes:

* **Capa de entrada**. Son las neuronas que introducen los patrones de entrada en la red.

* **Capa oculta**. Son las capas intermedias de la red.

* **Capa de salida**. Son las neuronas que determinan la salida de la red.


Vamos a realizar dicha red con la librer칤a Keras. Podemos personalizar con muchos hiperpar치metros las redes que construyamos. Antes de explicar c칩mo quedar치 dise침ado nuestro primer modelo, vamos a explicar otros conceptos b치sicos y comunes a todos.

Tenemos 8000 ejemplos de training y 3072 entradas, por lo que a priori es una red con cierta complejidad. Debemos tener cuidado con el n칰mero de capas y nodos que ponemos en la red, ya que podr칤amos crear una que tarde demasiado en computar y consuma excesiva memoria.


En el siguiente gr치fico podemos observar la distribuci칩n por clases de las im치genes. Con esto observamos c칩mo de sencilla ser치 la clasificaci칩n de las mismas, viendo c칩mo de clara es la separaci칩n entre clases.

```{r}
n = 8000
mask = sample(1:nrow(CIFAR10_10Kx),n)
pca = prcomp(CIFAR10_10Kx[mask,])
cols = rainbow(10)
colors = cols[1 + CIFAR10_10Ky$x[mask]]
plot(pca$x[,1],pca$x[,2],col=colors,pch=19,cex=0.3,
     xlab="1st PCA",ylab="2nd PCA",main=paste0("PCA plot, ",n," images MNIST"))
legend("topright",fill=cols,
       title="Digits",
       col=cols,
       legend=0:9,cex=0.6)
```

Vemos que se encuentran muy entremezcladas entre s칤, sin una divisi칩n clara, aunque ciertas clases se encuentran en zonas claras del espacio. Esto nos anticipa que se trata de un problema complejo y una red simple no ser치 capaz de sobreponerse a tal complejidad para separar las im치genes en clases.


Los aspectos que modificaremos de cada modelo son los siguientes:

* **Capas**. Tendremos una capa de entrada que modelar치 c칩mo se leen los datos del conjunto. Tendremos una capa de salida que tendr치 el mismo n칰mero de nodos que clases que clasificamos. Y finalmente tendremos una o dos capas ocultas que pueden contener un n칰mero cualquiera de nodos (no siendo 칠ste demasiado excesivo), y que se encargar치n de entender lo mejor posible el problema para clasificar las im치genes.
* **N칰mero de nodos**. Un n칰mero muy bajo de nodos en las capas ocultas har치 que la red no sepa clasificar adecuadamente, obteniendo as칤 malos resultados. Sin embargo, utilizar demasiados nodos en nuestra red provocar치 overfitting y un sobreetendimiento del problema, lo cual no nos interesa y conllevar치 un modelo demasiado complejo, con un excesivo consumo de tiempo y memoria. Adem치s, tampoco garantizar치 mejores resultados.
* **Funciones de activaci칩n**. Tenemos muchas, y no podemos probarlas todas. Vamos a mencionar algunas que son importantes y podr칤an dar un buen resultado:
    * *Softmax*. Esta es la funci칩n por activaci칩n que se utiliza com칰nmente en la capa de salida para clasificar los ejemplos en clases. Convierte un valor de regresi칩n en una clasificaci칩n, que al final es el tipo de problema que estamos resolviendo.
    * *Relu*. Se trata de una de las funciones m치s utilizadas en redes neuronales por sus buenos resultados en cualquier problema.
    * *Sigmoid y Tanh* (Hyperbolic Tangent). *Tanh* es una evoluci칩n de la funci칩n *sigmoid*, y est치 adquiriendo importancia con el paso del tiempo, ya que tambi칠n devuelve buenos resultados.
* **Funciones de optimizaci칩n**. Al igual que en el caso anterior, aqu칤 tenemos otra gran variedad de opciones. La m치s utilizada es *SGD* (Stochastic gradient descent), y hay otras como *Adagrad*, *Adadelta*, y *Adam* que tienen cierta reputaci칩n. Para no agrandar el tama침o de este Markdown se han realizado pruebas a parte y la funci칩n que mejor se comportaba con nuestro problema fue *Adam*, por lo tanto, es la que utilizaremos en todo el documento.

A parte de los hiperpar치metros y la arquitectura que podemos modificar en los modelos tambi칠n podemos indicar como entrenar los modelos. Esto lo realizamos con los *epochs* y el *batch_size*. Los *epochs*, 칠pocas en espa침ol, indican el n칰mero de veces que se entrena el conjunto de datos con la red neuronal. Son el n칰mero de "pasadas" que se realizan a los datos. El *batch_size* indica el n칰mero de ejemplos que contendr치 cada bloque de ejemplos que se entrena, ya que el conjunto de datos se puede dividir en varias partes para entrenar de forma m치s espec칤fica y menos general. Cuantas m치s divisiones realicemos, mayor ser치 el tiempo de c칩mputo y el riesgo de overfitting, pero tambi칠n la red aprender치 mejor.


Comenzaremos con un primer modelo sencillo, con la capa de entrada, una capa oculta y una capa de salida. **La capa de entrada tendr치 3072 nodos, adapt치ndose a las 3072 variables de nuestro problema**. **La capa oculta tendr치 tan solo 32 nodos**. Queremos comenzar con una arquitectura muy sencilla y ver que resultados arroja. Este es un punto de partida discreto, aunque quiz치 suficiente, ya que si cuando aumentemos este n칰meor no mejoramos lo suficiente veremos que ser치 igual cuanto m치s aumentemos la complejidad de la red porque no va a obtener mejores resultados para el problema de clasificaci칩n de im치genes que estamos tratando. **La capa de salida tendr치 10 nodos, uno por cada clase del problema**.

Ya que vamos a realizar diez ejecuciones por modelo queremos que la red sea r치pida, y como la potencia de c칩mputo lo permite, **comenzaremos con un batch_size elevado, de 768**. As칤, los datos se dividen en cuatro conjuntos en cada *epoch*. **Utilizaremos 100 epochs** para obtener el m치ximo rendimiento posible desde el principio y no quedarnos cortos, aunque eso cueste algo m치s de tiempo de c칩mputo, ya que un n칰mero adecuado de *epochs* devuelve un mejor resultado que un bajo n칰mero de *batch_size*.

Las funci칩n de activaci칩n que se utilizar치 en este primer modelo ser치 **relu**.


Una vez explicado el primer modelo, vamos a crearlo, compilarlo, entrenarlo, y validarlo. Para mejorar la simpleza, legibilidad y mantenimiento del c칩digo se ha concentrado el c칩digo que muestra los resultados y los imprime en forma de gr치ficas en las siguientes dos funciones:

```{r}
plotResults1 <- function(historys, history){
  loss = list()
  val_loss = list()
  accuracy = list()
  val_accuracy = list()
  
  for (i in 1:nConj){
    loss[[i]] = historys[[i]][[2]][[1]]  # historys[[i]]$metrics$loss
    val_loss[[i]] = historys[[i]][[2]][[3]]  # historys[[i]]$metrics$val_loss
    accuracy[[i]] = historys[[i]][[2]][[4]]  # historys[[i]]$metrics$accuracy
    val_accuracy[[i]] = historys[[i]][[2]][[2]]  # historys[[i]]$metrics$val_accuracy
  }
  
  history$metrics$loss = rowMeans(cbind(array(as.numeric(unlist(loss)), dim = c(100,1,10))[,,]))
  history$metrics$val_loss = rowMeans(cbind(array(as.numeric(unlist(val_loss)), dim = c(100,1,10))[,,]))
  history$metrics$accuracy = rowMeans(cbind(array(as.numeric(unlist(accuracy)), dim = c(100,1,10))[,,]))
  history$metrics$val_accuracy = rowMeans(cbind(array(as.numeric(unlist(val_accuracy)), dim = c(100,1,10))[,,]))
  
  cat("Los errores de entrenamiento y evaluaci칩n finales son", history$metrics$loss[epochs],"y",
      history$metrics$val_loss[epochs],"\n")
    
  cat("Los valores de accuracy de entrenamiento y evaluaci칩n finales son", history$metrics$accuracy[epochs],"y",
      history$metrics$val_accuracy[epochs],"\n")
  
  
  plot(history)
}

plotResults2 <- function(history){
  vymax = max(c(history$metrics$loss,history$metrics$val_loss))
  plot(history$metrics$loss,main="Training/Validation errors",col="blue",
       type="l",xlab="Epochs",ylab="Loss",ylim=c(0,vymax))
  lines(history$metrics$val_loss,col="red")
}
```

No se han unido en una 칰nica funci칩n porque R no mostraba ambas gr치ficas, tan solo una.

Definimos las variables comunes a todos los modelos:

```{r}
epochs = 100
batch_size = 768
input_shape = c(3072)
num_classes = 10
```


Primer modelo:

```{r}
for (i in 1:nConj){
  model = keras_model_sequential() 
  model %>% 
    layer_dense(units = 32, activation = 'relu', input_shape = input_shape) %>%
    layer_dense(units = num_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  act = model %>% fit(
    datos$train$x[[i]], datos$train$y[[i]], 
    epochs = epochs, 
    batch_size = batch_size, 
    validation_data = list(datos$test$x[[i]], datos$test$y[[i]]),
    verbose = 0
  )
  
  if (i == 1){
    # Crear lista
    historys = list(act)
    history = act
    summary(model)
  } else {
    historys[[i]] = act
  }
  
}
```

Este modelo genera casi 100.000 par치metros. No es una cifra muy elevada pero si algo significante. 쯇or qu칠 obtenemos tantos par치metros si hemos apuntado que nuestra red es simple? Por la gran cantidad de nodos que tiene la red en la entrada. Esto hace que la red que se vaya a construir a partir de ah칤, por pocos nodos que sea, tendr치 un n칰mero de par치metros considerable.

```{r}
plotResults1(historys, history)
plotResults2(history)
```

El valor objetivo en la pr치ctica con este tipo de redes MLP es 0.4 en el accuracy. Y los resultados obtenidos no son muy halagadores. Tenemos margen de mejora, y debemos ver c칩mo hacerlo.

Lo que tambi칠n se observa, mirando tanto los datos como las gr치ficas, es que no se produce overfitting aunque estamos cerca. Esto es detectado en primer lugar porque los valores de accuracy son dispares entre entrenamiento y evaluaci칩n. Tambi칠n hay dispariedad entre los valores de loss. En la gr치fica adem치s vemos como las curvas de los datos de entrenamiento (la roja) se separa de la azul.

Sin embargo, esto no es algo raro, es normal que se obtengan mejores resultados de los datos del conjunto de entrenamiento, siempre y cuando no se vea afectado el resultado de validaci칩n, y en este caso no empeora en ning칰n momento. Por esta raz칩n no vamos a tratarlo por el momento, pero es posible que tengamos que hacerlo m치s adelante.

Para conseguir un mejor resultado de la red vamos a realizar dos cambios. En primer lugar, vamos a aumentar el **n칰mero de nodos de la capa oculta a 256**, lo cual aumentar치 la complejidad de la red adecu치ndose m치s al problema. Estamos tratando un problema muy complejo como hemos podido analizar viendo la gr치fica dibujada al principio de este apartado, y la red que hemos construido es muy sencilla. En segundo lugar, vamos a cambiar la funci칩n de activaci칩n para utilizar **tanh** y ver si esta es mejor que relu. En caso contrario, volveremos a ella o probaremos con otra.


```{r}
for (i in 1:nConj){
  model = keras_model_sequential() 
  model %>% 
    layer_dense(units = 256, activation = 'tanh', input_shape = input_shape) %>%
    layer_dense(units = num_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  act = model %>% fit(
    datos$train$x[[i]], datos$train$y[[i]], 
    epochs = epochs, 
    batch_size = batch_size, 
    validation_data = list(datos$test$x[[i]], datos$test$y[[i]]),
    verbose = 0
  )
  
  if (i == 1){
    # Crear lista
    historys = list(act)
    history = act
    summary(model)
  } else {
    historys[[i]] = act
  }
  
}
```

Lo primero que se observa es claramente un aumento de la complejidad de la red. Pensemos que hemos multiplicado por 8 el n칰mero de nodos de la capa oculta que ten칤amos anteriormente, y eso se traduce en muchos par치metros m치s. El tiempo de c칩mputo ser치 mucho mayor que antes pero esto es algo necesario dado el resultado obtenido. Dado el gran n칰mero de variables de la entrada, incluso podr칤amos decir que no es una cifra disparatada.

```{r}
plotResults1(historys, history)
plotResults2(history)
```

Hemos obtenido un resultado muy positivo, cumpliendo el objetivo marcado anteriormente. Hemos sin embargo incrementado el problema del overfitting, siendo ahora ya real. Lo podemos observar sobre todo en la segunda gr치fica, que solo contempla la diferencia de error. La l칤nea azul solo baja mientras que la roja, a partir del epoch 55 aproximadamente aumenta ligeramente. Tenemos dos maneras de tratar el overfitting, una es utilizar un **regularizador L1, L2 o L1 y L2**. Otra es utilizar la t칠cnica de dropout.

Lo que hacen las t칠cnicas de regulaci칩n es reducir la importancia de ciertos par치metros (pesos) en la red, consiguiendo as칤 que todos tengan una importancia y no se centre excesivamente en ciertos par치metros. Con el **dropout** eliminamos ciertos par치metros aleatoriamente en funci칩n del ratio indicado. En esta red vamos a optar por la segunda opci칩n, m치s adecuada para redes neuronales complejas como la nuestra. **El valor por defecto que se utiliza con esta t칠nica suele ser 0.5**, que equivale a descartar el 50% de los par치metros. En funci칩n de c칩mo responda le asignaremos m치s o menos porcentaje.

```{r}
for (i in 1:nConj){
  model = keras_model_sequential() 
  model %>% 
    layer_dense(units = 256, activation = 'tanh', input_shape = input_shape) %>%
    layer_dropout(rate=0.5) %>%
    layer_dense(units = num_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  act = model %>% fit(
    datos$train$x[[i]], datos$train$y[[i]], 
    epochs = epochs, 
    batch_size = batch_size, 
    validation_data = list(datos$test$x[[i]], datos$test$y[[i]]),
    verbose = 0
  )
  
  if (i == 1){
    # Crear lista
    historys = list(act)
    history = act
    summary(model)
  } else {
    historys[[i]] = act
  }
  
}
```

```{r}
plotResults1(historys, history)
plotResults2(history)
```

Se sigue produciendo overfitting aunque en menor medida, por lo que para el siguiente modelo aumentaremos ese ratio a 0.65. Hemos visto que aumentar la complejidad de la red ha mejorado el resultado, lo cual es evidente porque en el estudio del problema hemos indicado que se trataba de un problema complejo. Sin embargo, para la cantidad de ratio en el que hemos aumentado el n칰mero de nodos la mejora obtenida ha sido muy poca. Esto nos puede hacer indicar que quiz치 tengamos que crear una red muy compleja para obtener un buen resultado o que directamente esta red no es adecuada para resolver este tipo de problemas.

Vamos a seguir aumentando la complejidad de esta primera capa para probar nuestro punto. Ahora tendremos el doble de nodos, 512, y en funci칩n de c칩mo se comporte tomaremos una decisi칩n. Al tener m치s nodos es l칩gico aumentar el droput, por lo que **subiremos el ratio al 0.75**.

```{r}
for (i in 1:nConj){
  model = keras_model_sequential() 
  model %>% 
    layer_dense(units = 512, activation = 'tanh', input_shape = input_shape) %>%
    layer_dropout(rate=0.75) %>%
    layer_dense(units = num_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  act = model %>% fit(
    datos$train$x[[i]], datos$train$y[[i]], 
    epochs = epochs, 
    batch_size = batch_size, 
    validation_data = list(datos$test$x[[i]], datos$test$y[[i]]),
    verbose = 0
  )
  
  if (i == 1){
    # Crear lista
    historys = list(act)
    history = act
    summary(model)
  } else {
    historys[[i]] = act
  }
  
}
```

La primera consecuencia de esto es que el n칰mero de par치metros de nuestra red es ahora el doble que antes. Lo cual es l칩gico evidentemente ya que hemos doblado la cantidad de nodos que tenemos en la capa oculta. Estamos alcanzando ya n칰meros demasiados altos para una red neuronal, lo cual no es nada positivo, y no deber칤amos seguir aumentando la complejidad de la misma.

```{r}
plotResults1(historys, history)
plotResults2(history)
```

No tenemos absolutamente nada de overfitting, por lo que incluso podr칤amos reducir un poco el ratio de dropout. Quiz치 con m치s epochs, 150 por ejemplo, podr칤amos seguir entrenando la red y conseguir un resultado a칰n mejor, ya que como podemos observar en las gr치ficas, las l칤neas de accuracy siguen en crecimiento en el epoch n칰mero 100. Sin embargo, aunque aument치ramos el n칰mero de 칠pocas, tampoco se conseguir칤a mucha mejora, porque el crecimiento de la red es ya muy lento y a cambio tendr칤amos mucho m치s tiempo de c치lculo, por lo que si buscamos una relaci칩n complejidad/resultado no sale rentable.

Si aumentar el n칰mero de nodos en la primera capa oculta no mejora nuestra red, quiz치 lo haga colocando una segunda capa oculta conectada completamente a la primera. Por lo tanto, **volvemos a la red en la que ten칤amos 256 nodos en la primera capa oculta y creamos una segunda capa con 128 nodos**. La mitad que la primera. Poner pocos no ser칤a muy resolutivo ya que podr칤amos obtener un mal resultado, y poner muchos aumentar칤a mucho el tiempo de c칩mputo.

Si mejoramos el resultado seguiremos experimentado con esta segunda capa, si por el contrario lo empeora, concluiremos que, a priori, una segunda capa oculta no mejora nuestra red.

**A esta segunda capa le colocaremos un droput de 0.5**.

```{r}
for (i in 1:nConj){
  model = keras_model_sequential() 
  model %>% 
    layer_dense(units = 256, activation = 'tanh', input_shape = input_shape) %>%
    layer_dropout(rate=0.65) %>%
    layer_dense(units = 128, activation = 'tanh') %>%
    layer_dropout(rate=0.5) %>%
    layer_dense(units = num_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  act = model %>% fit(
    datos$train$x[[i]], datos$train$y[[i]], 
    epochs = epochs, 
    batch_size = batch_size, 
    validation_data = list(datos$test$x[[i]], datos$test$y[[i]]),
    verbose = 0
  )
  
  if (i == 1){
    # Crear lista
    historys = list(act)
    history = act
    summary(model)
  } else {
    historys[[i]] = act
  }
  
}
```

Con este aumento de complejidad no se dispara el n칰mero de par치metros lo cual es muy positivo. Tener muchos nodos en la segunda capa oculta es mucho menos costoso computacionalmente que tenerlos en la primera.

```{r}
plotResults1(historys, history)
plotResults2(history)
```

**El resultado es peor**. No solo no hay mejora sino que empeoramos, y aunque la curva de accuracy sigue siendo ascendente en el epoch 100 nos ocurre lo mismo que en el modelo anterior, no conseguir칤amos una gran mejora aumentando el n칰mero de 칠pocas. Un punto positivo es que no se produce overfitting.


## Mejor modelo

El mejor modelo encontrado de las redes MLP es aquel que solamente ten칤a una capa oculta con 256 nodos. El tratamiento del overfitting se produc칤a con la t칠cnica Dropout, a침adiendo una capa de este tipo a continuaci칩n de la capa oculta descrita y con un ratio de descarte de 0.65. La funci칩n de activaci칩n que mejor se comporta es *tanh* y la de optimizaci칩n *Adam*.




# Redes convolucionales

Son un tipo de las redes feed forward fully connected (FFFC). Su idea general es una red MLP como las construidas anteriormente, a las que se le a침aden filtros que transforman las im치genes originales realiz치ndoles alg칰n tipo de modificaci칩n (con multiplicaci칩n de matrices) para su mejor entendimiento y procesamiento en la red.

Este tipo de redes son normalmente usadas para el reconocimiento y trabajo con im치genes en redes neuronales, ya que usualmente dan un buen resultado.

Tal y como se explica en el archivo de pr치cticas de las redes convolucionales, analizar una imagen con una red neuronal MLP es muy costoso, ya que se generan demasiados par치metros incluso para una imagen simple. En nuestro caso, el mejor modelo encontrado tiene alrededor de 800.000 par치metros, lo cual es una cifra muy elevada para una red neuronal, con un alto coste de memoria y de tiempo. Las redes convolucionales permiten reducir la complejidad de la red, no solo mejorando as칤 el uso de memoria sino tambi칠n el resultado final obtenido.

Antes de dise침ar el primer modelo de red neuronal vamos a cambiar la estructura de los datos. En las redes MLP la entrada era un array de dos dimensiones (o una matriz) con 8000 entradas de 3072 variables que representaban los colores de los p칤xeles de la imagen. Ahora, tenemos que crear un array de 4 dimensiones. La primera dimensi칩n, al igual que en el caso anterior tendr치 los 8000 ejemplos. Despu칠s tendremos una dimensi칩n donde se representan las dilas, despu칠s, otra donde se representan las columnas, y finalmente tendremos tres valores que representan el RGB de cada p칤xel.

El tama침o de las im치genes son 32x32, es decir, 32 p칤xeles de altura y 32 de anchura:

```{r}
height = 32
width = 32
```

Recorremos todos los conjuntos de entrenamiento y de test, cambiando su estructura a la mencionada anteriormente:

```{r}
for (i in 1:nConj){
  datos[[1]][[1]][[i]] = array_reshape(datos[[1]][[1]][[i]], c(8000, 32, 32, 3))
  datos[[2]][[1]][[i]] = array_reshape(datos[[2]][[1]][[i]], c(2000, 32, 32, 3))
}
```

Una vez cambiada la representaci칩n de los datos en la variable *datos*, cambiamos tambi칠n la variable input_shape que indica al modelo c칩mo se representar치 la entrada. Indicamos que cada ejemplo tendr치 32x32x3.

```{r}
input_shape = c(32, 32, 3)
```

Ahora si, vamos a definir el primer modelo de redes convolucionales. La estrategia que se sigue para construir los modelos ha sido desarrollada a partir de las recomendaciones del siguiente art칤culo de la web pyimagesearch.com: https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/.

Trabajaremos con dos tipos de capas.

* Las **capas de convoluci칩n** aplican filtros a las im치genes para transformarlas y que sean m치s sencillas de tratar en las capas ocultas de la red. Los par치metros que se pueden modificar en esta capa son los siguientes:

    * El **n칰mero de nodos o de filtros** que se aplicar치n a la imagen. Cada uno se encargar치 de modelar o modificar un aspecto de la imagen. Cuanto antes se encuentre la capa de convoluci칩n, menos filtros son necesarios, y ocurre lo contrario al final. Lo com칰n y recomendado es utilizar n칰meros potencia de dos, y son habituales los rangos [32,64,128] y [256,512,1024]. Comenzaremos con dos capas de convoluci칩n con 32 y 64 filtros.
    * El siguiente par치metros es el **tama침o del kernel**, es decir, de la matriz que se aplica sobre la imagen. Para im치genes de un tama침o igual o mayor a 128x128 se comienza utilizando un tama침o de (7x7) o (5x5) y se va disminuyendo conforme se reduce la complejidad de la red gracias a las capas de pool. En este caso, ya que las im치genes son tan solo de 32x32 se utilizar치 (3x3) como m치ximo, y en algunos casos se utilizar치 (1x1).
    * Otro par치metros que podemos modificar es el llamado ***strides***, que es el n칰mero de p칤xels que avanza la matriz que va tratando la imagen. En este caso iremos avanzando p칤xel a p칤xel, tanto por el ancho como por el alto de la imagen.
    * El par치metro ***padding*** se encarga de rellenar los bordes de la figura artificialmente. Con el valor *valid* estar칤amos reduciendo las dimensiones de la imagen en aquellos casos que as칤 se produzca. Con *same* se rellena la imagen artificalmente para que conserve su tama침o. No tenemos intenci칩n en que la imagen conserve su tama침o, por lo que estableceremos esta variable al valor *valid*, que es adem치s el valor por defecto de esta capa en Keras.
    * Al igual que ocurr칤a en las capas ocultas aqu칤 tambi칠n tenemos que elegir una **funci칩n de activaci칩n**. *Relu* funciona normalmente con buenos resultados, sin embargo, dadas nuestras satisfactorias pruebas con *tanh*, seguiremos us치ndola por el momento.

* Los otros tipos de capas son las capas de pooling. Estas se encargan de ir reduciendo la complejidad de la red, y se suelen colocar a continuaci칩n de las capas de convoluci칩n. En este caso se colocar치n dos, cada una detr치s de la capa de convoluci칩n. El tama침o ser치 de (2x2).

En este primer modelo realizaremos poco tratamiento de la imagen a la entrada. Tan solo aplicaremos 32 filtros al principio con un kernel de (3x3) y 64 filtros a continuaci칩n, esta vez con un kernel de (1x1). Al terminar cada capa, se aplicar치 una capa de pooling que reducir치 la complejidad de la imagen, para poder as칤 aplicar mejor los siguientes filtros.

Las capas de convoluci칩n se encargan de extraer caracter칤sticas de las im치genes. Esto hace que la tarea de clasificar en clases sea m치s sencilla para la red, ya que tiene resaltadas ciertas caracter칤sticas de las im치genes que facilitar치 el trabajo de distinguirlas entre si. Al reducir la complejidad del problema, debe hacerlo tambi칠n la capa oculta. Antes ten칤amos 256 nodos, y vimos que aumentar ese n칰mero aumenta la complejidad de la red y no funciona bien con el problema, ya que no clasifica correctamente. 64 es una cifra razonable, no muy alejada de ese 256 y no es una cifra baja. Para comenzar es un valor medio correcto.

Las funciones de optimizaci칩n y activaci칩n que utilizaremos ser치n las mismas que en el caso de las redes MLP, ya que se comportan bien en este problema.

Procedemos ahora a compilar el primer modelo, a entrenarlo y a evaluarlo:

```{r}
for (i in 1:nConj){
  # Cargamos la librer칤a de keras para la funci칩n 'to_categorical'.
  library(keras)
  
  model = keras_model_sequential() 
  model %>% 
    layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'tanh', padding = "valid", strides = c(1L, 1L), input_shape = input_shape) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 64, kernel_size = c(1,1), activation = 'tanh', padding = "valid", strides = c(1L, 1L)) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_flatten() %>%
    layer_dense(units = 64, activation = 'tanh') %>%
    layer_dropout(rate=0.2) %>%
    layer_dense(units = num_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  act = model %>% fit(
    datos$train$x[[i]], datos$train$y[[i]], 
    epochs = epochs, 
    batch_size = batch_size, 
    validation_data = list(datos$test$x[[i]], datos$test$y[[i]]),
    verbose = 0
  )
  
  if (i == 1){
    # Crear lista
    historys = list(act)
    history = act
    summary(model)
  } else {
    historys[[i]] = act
  }
  
}
```

El modelo compilado tiene pocos par치metros en comparaci칩n con lo que ocurr칤a con las redes MLP, tan solo 204.426. Esto conlleva un mejor uso de la memoria de la m치quina, aunque el tiempo de ejecuci칩n ha aumentado.

```{r}
plotResults1(historys, history)
plotResults2(history)
```

Los resultados extra칤dos no ser치n valorados debido al overfitting que se ha producido. Es demasiado acentuado para poder obtener conclusiones. Por lo tanto, la primera modificaci칩n que realizaremos ser치 ejecutar la misma red que antes a침adiendo una capa de dropout al final de ambas capas de convoluci칩n. El rate debe ser muy elevado, ya que el overfitting ha sido muy pronunciado.

```{r}
for (i in 1:nConj){
  # Cargamos la librer칤a de keras para la funci칩n 'to_categorical'.
  library(keras)
  
  model = keras_model_sequential() 
  model %>% 
    layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'tanh', padding = "valid", strides = c(1L, 1L), input_shape = input_shape) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 64, kernel_size = c(1,1), activation = 'tanh', padding = "valid", strides = c(1L, 1L)) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_dropout(rate=0.7) %>%
    layer_flatten() %>%
    layer_dense(units = 64, activation = 'tanh') %>%
    layer_dropout(rate=0.2) %>%
    layer_dense(units = num_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  act = model %>% fit(
    datos$train$x[[i]], datos$train$y[[i]], 
    epochs = epochs, 
    batch_size = batch_size, 
    validation_data = list(datos$test$x[[i]], datos$test$y[[i]]),
    verbose = 0
  )
  
  if (i == 1){
    # Crear lista
    historys = list(act)
    history = act
    summary(model)
  } else {
    historys[[i]] = act
  }
  
}
```

```{r}
plotResults1(historys, history)
plotResults2(history)
```

Ahora si tenemos un resultado que podemos analizar. Es claramente mejor que lo que obten칤amos con las redes MLP, lo cual era lo esperado, ya que sab칤amos de antemano que tratar las im치genes con capas de convoluci칩n iba a facilitar la tarea a la red. Vamos ahora a seguir realizando pruebas en la red para obtener una mejor clasificaci칩n.

Realizaremos varios cambios. En primer lugar, dado que extraer estas caracter칤sticas de la imagen es positivo, seguiremos haci칠ndolo con una nueva capa de convoluci칩n. Vamos a a침adir una de 128 filtros a continuaci칩n de las dos anteriores. Adem치s, el kernel de la capa de 64 filtros ser치 ahora de (3x3) y el de 128 de (1x1). Para no caer de nuevo en un problema de overffiting, a침adiremos una nueva capa de droput al final de la de convoluci칩n de 128 filtros. Y entre ambas, una de pooling, para reducir a칰n m치s la complejidad de la imagen.

La capa oculta la dejaremos igual. Dado que las im치genes ser치n ahora m치s simples, incluso podr칤amos disminuir la cantidad de nodos de la misma. Sin embargo, ya que no es un n칰mero elevado lo dejaremos como anteriormente.

```{r}
for (i in 1:nConj){
  # Cargamos la librer칤a de keras para la funci칩n 'to_categorical'.
  library(keras)
  
  model = keras_model_sequential() 
  model %>% 
      layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'tanh', padding = "valid", strides = c(1L, 1L), input_shape = input_shape) %>%
      layer_max_pooling_2d(pool_size = c(2, 2)) %>%
      layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'tanh', padding = "valid", strides = c(1L, 1L)) %>%
      layer_max_pooling_2d(pool_size = c(2, 2)) %>%
      layer_dropout(rate=0.7) %>%
      layer_conv_2d(filters = 128, kernel_size = c(1,1), activation = 'tanh', padding = "valid", strides = c(1L, 1L)) %>%
      layer_max_pooling_2d(pool_size = c(2, 2)) %>%
      layer_dropout(rate=0.3) %>%
      layer_flatten() %>%
      layer_dense(units = 64, activation = 'tanh') %>%
      layer_dropout(rate=0.2) %>%
      layer_dense(units = num_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  act = model %>% fit(
    datos$train$x[[i]], datos$train$y[[i]], 
    epochs = epochs, 
    batch_size = batch_size, 
    validation_data = list(datos$test$x[[i]], datos$test$y[[i]]),
    verbose = 0
  )
  
  if (i == 1){
    # Crear lista
    historys = list(act)
    history = act
    summary(model)
  } else {
    historys[[i]] = act
  }
  
}
```

En primer lugar observamos que el n칰mero de par치metros es pr치cticamente la mitad que antes. Esto ocurre porque hemos a침adido otra capa de dropout al final de la capa de 128 filtro. Esto reduce la mitad la complejidad de la imagen, y provoca que tengamos menos nodos totalmente conectados en la capa oculta. Esto es algo positivo, ya que consumiremos menos recursos, y como hemos dicho anteriormente, podr칤amos haber reducido el n칰mero de nodos de la capa oculta.

```{r}
plotResults1(historys, history)
plotResults2(history)
```

Encontramos un poco de overfitting, sin embargo no es excesivo y no empeora los resultados, ya que vemos que la curva de accuracy sigue siendo ascendente en el epoch n칰mero 100, y lo mismo ocurre con la curva de loss, pero en sentido descendente. Lo que tambi칠n nos indica esto es que con un n칰mero mayor de 칠pocas y con un tratamiento m치s agresivo del overfitting obtendr칤amos un mejor resultado.

Ya que a침adir esta nueva capa convolucional no ha mejorado nuestra red, tenemos que intentar encontrar otra manera de a침adir capas de convoluci칩n que a침alicen la imagen para un mejor comportamiento de la red MLP. Eliminamos por tanto la capa convolucional de 128 filtros y a침adimos nuevas capas, de 32 y 64 filtros. Las colocaremos de manera sucesiva con las anteriores. Esto doblar치 el tratamiento que se le estaba dando a la imagen anteriormente. No se a침adir치 una nueva capa de pooling, por si eso simplificaba demasiado la imagen, y dejaremos el resto de la red igual a excepci칩n de la capa de dropout.

Anteriormente nos hemos visto obligados a descartar muchos par치metros, ya que ten칤amos un overfitting muy alto. Si el ratio era entonces 0.7, ahora deber칤a ser como m칤nimo, y eso supone descartar el 80% de los par치metros. Algo muy elevado. Para no llegar a ese extremo utilizaremos dos capas de dropout en lugar de una. As칤, repartimos los par치metros que se descartan y no tenemos un punto muy agresivo en la red. Ambas capas se colocar치n despu칠s de las capas de pooling y su ratio ser치 de 0.6.

```{r}
for (i in 1:nConj){
  # Cargamos la librer칤a de keras para la funci칩n 'to_categorical'.
  library(keras)
  
  model = keras_model_sequential() 
  model %>% 
    layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'tanh', padding = "valid", strides = c(1L, 1L), input_shape = input_shape) %>%
    layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'tanh', padding = "valid", strides = c(1L, 1L)) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_dropout(rate=0.6) %>%
    layer_conv_2d(filters = 64, kernel_size = c(1,1), activation = 'tanh', padding = "valid", strides = c(1L, 1L)) %>%
    layer_conv_2d(filters = 64, kernel_size = c(1,1), activation = 'tanh', padding = "valid", strides = c(1L, 1L)) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_dropout(rate=0.6) %>%
    layer_flatten() %>%
    layer_dense(units = 64, activation = 'tanh') %>%
    layer_dropout(rate=0.2) %>%
    layer_dense(units = num_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  act = model %>% fit(
    datos$train$x[[i]], datos$train$y[[i]], 
    epochs = epochs, 
    batch_size = batch_size, 
    validation_data = list(datos$test$x[[i]], datos$test$y[[i]]),
    verbose = 0
  )
  
  if (i == 1){
    # Crear lista
    historys = list(act)
    history = act
    summary(model)
  } else {
    historys[[i]] = act
  }
  
}
```

El n칰mero de par치metros ha aumentado en un grado menor, lo cual no nos preocupa ni supondr치 un necesario incremento en el n칰mero de nodos de la capa oculta.

```{r}
plotResults1(historys, history)
plotResults2(history)
```

El resultado es parecido al obtenido con la primera red convolucional a la que le aplicamos dropout. Por lo tanto esta no es la manera correcta de analizar las im치genes, ya que hemos obtenido un resultado similar con un mayor consumo de recursos. Debe haber otras maneras de conseguir analizar las im치genes y extraer las car치cter칤sticas y que esto suponga una mejor clasificaci칩n de las mismas. Este no es el camino, quiz치 con un n칰mero reducido de capas y un mayor n칰mero de filtros se podr칤a conseguir, aunque a priori tampoco nos parec칤a una buena opci칩n, ya que las im치genes CIFAR-10 tienen una resoluci칩n de 32x32 p칤xeles, eso no las hace complejas.


## Mejor modelo

El mejor modelo encontrado en este caso es curiosamente el m치s sencillo de los creados. El que tan solo tiene dos capas convolucionales con 32 y 64 filtros respectivamente. Cuando se ha intentado aumentar el n칰mero de capas se han obtenido peores resultados. Esto nos deja que las posibles mejoras que se podr칤an haber realizado son: aumentar el n칰mero de filtros en dichas capas o reducirlo, combinando esto con la incorporaci칩n de otras capas. Las funciones de activaci칩n y optimizaci칩n son las mismas que utilizadas en las redes MLP, y el resto de hiperpar치metros ya fueron explicados al principio de este apartado.




# Conclusiones

Hemos visto que las redes MLP que no son muy efectivas para el reconocimiento de im치genes. El accuracy que podemos conseguir como m치ximo no sobrepasa el 0.43 o 0.44, lo cual es un resultado muy pobre, ya que ni el 50% de las fotograf칤as se est치 clasificando correctamente. Podr칤amos obtener mejores resultados realizando m치s pruebas o con un estudio m치s detallado del problema, pero es mejor invertir ese tiempo a otro tipo de red que se adapte mejor a este problema.

Las capas de convoluci칩n extraen caracter칤sticas de las im치genes que las hacen m치s f치ciles de clasificar para la red. Las redes MLP no son capaces de procesar informaci칩n tan compleja y clasificarla con un resultado aceptable. De hecho, aunque aumentemos el n칰mero de nodos y capas de estas redes no son capaces de entender la naturaleza de cada imagen, y los resultados no son mejores.

Por eso, es evidente que las capas convolucionales aportan un valor extra a la red, y proporcionan informaci칩n sesgada a las capas ocultas, las cuales son capaces de separar las im치genes de una manera m치s intuitiva y acertada que antes.

Sin embargo, con las redes convolucionales ha ocurrido algo parecido que con las MLP. Aumentar el n칰mero de capas tampoco ha resultado positivo en ning칰n caso. Podr칤amos haber probado con mayor n칰mero de filtros, cambiando las capas de pooling, etc. Sin embargo, sabemos, viendo los buenos resultados obtenidos desde el primer momento, y que hay muchos m치s par치metros que combinar que en las redes MLP, que estas redes tienen margen de mejora y se podr칤a conseguir un resultado a칰n mejor, cercano o superando un accuracy de 0.6.

Las redes convolucionales, adem치s, son m치s complejas en si mismas. No solo incluyen la arquitectura y los hiperpar치metros de una red MLP, si no que aplican matrices a las im치genes para transformarlas consiguiendo una transformaci칩n del conjunto de datos, reducen la complejidad de las im치genes y as칤 tambi칠n la del problema, etc. Esto las convierte en redes m치s potentes que las MLP y con un mayor potencial a explotar.

Con esto queda concluido este documento en el que hemos expuesto dos arquitecturas de redes neuronales diferentes a un conjunto de im치genes para comprobar qu칠 tecnolog칤a se comportaba mejor para la tarea de clasificar las mismas, con una aproximaci칩n desde un punto de vista pr치ctico.




# Bibliograf칤a

Para la gesti칩n del almacenamiento y transformaci칩n de los datos:

* Documentaci칩n de R, *array_reshape*. URL: https://www.rdocumentation.org/packages/reticulate/versions/1.16/topics/array_reshape

* Documentaci칩n de R, *unlist*. URL: https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/unlist

* Documentaci칩n de R, *cbind*. URL: https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/cbind

* Estructuras de datos en R. Alberto Mu침oz Garc칤a. URL: http://ocw.uc3m.es/estadistica/aprendizaje-del-software-estadistico-r-un-entorno-para-simulacion-y-computacion-estadistica/estructuras-de-datos-en-r

* Transparencias Universidad de Sevilla. Francisco J. Romero Campero. URL: https://www.cs.us.es/~fran/TIB/semana3.pdf

* A침adir elementos al final de una lista en un bucle. Stack Overflow. URL: https://stackoverflow.com/questions/26508519/how-to-add-elements-to-a-list-in-r-loop

* Convertir lista a matriz. Stack Overflow. URL: https://stackoverflow.com/questions/37433509/convert-list-to-a-matrix-or-array

Informaci칩n de redes neuronales:

* Intermediate Topics in Neural Networks. Matthew Stewart. URL: https://towardsdatascience.com/comprehensive-introduction-to-neural-network-architecture-c08c6d8e5d98

* A guide to an efficient way to build neural network architectures- Part II: Hyper-parameter selection and tuning for Convolutional Neural Networks using Hyperas on Fashion-MNIST. Shashank Ramesh. URL: https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-ii-hyper-parameter-42efca01e5d7

* Selecting the Best Architecture for Artificial Neural Networks. Ahmed Gad. URL: https://heartbeat.fritz.ai/selecting-the-best-architecture-for-artificial-neural-networks-7b051f775b4

* How to decide neural network architecture?. Stack Exchange. URL: https://datascience.stackexchange.com/questions/20222/how-to-decide-neural-network-architecture

* How to choose the number of hidden layers and nodes in a feedforward neural network?. Stack Exchange. URL: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw

* How to Reduce Overfitting With Dropout Regularization in Keras. Jason Brownlee. URL: https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/

* Dropout Regularization in Deep Learning Models With Keras. Jason Brownlee. URL: https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/

* Keras Conv2D and Convolutional Layers. Adrian Rosebrock. URL: https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/

* Understanding deep Convolutional Neural Networks 游녜 with a practical use-case in Tensorflow and Keras. Ahmed Besbes. URL: https://www.ahmedbesbes.com/blog/introduction-to-cnns

Otros proyectos de CIFAR:

* Proyecto: Reconocimiento de Objetos de en Fotograf칤as con CIFAR-10. Blog Unipython. URL: https://unipython.com/proyecto-reconocimiento-de-objetos-de-en-fotografias-con-cifar-10/

* How to Develop a CNN From Scratch for CIFAR-10 Photo Classification. Jason Brownlee. URL: https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/